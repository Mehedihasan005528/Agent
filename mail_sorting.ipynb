{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0l4rcTWrPt"
      },
      "source": [
        "# Alfred the Mail Sorting Butler: A LangGraph Example\n",
        "\n",
        "In this notebook, **we're going to build a complete email processing workflow using LangGraph**.\n",
        "\n",
        "This notebook is part of the <a href=\"https://www.hf.co/learn/agents-course\">Hugging Face Agents Course</a>, a free course from beginner to expert, where you learn to build Agents.\n",
        "\n",
        "![Agents course share](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png)\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "In this notebook, you'll learn how to:\n",
        "1. Set up a LangGraph workflow\n",
        "2. Define state and nodes for email processing\n",
        "3. Create conditional branching in a graph\n",
        "4. Connect an LLM for classification and content generation\n",
        "5. Visualize the workflow graph\n",
        "6. Execute the workflow with example data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jCKPiFIkWrPw"
      },
      "outputs": [],
      "source": [
        "# Install the required packages\n",
        "%pip install -q langgraph langchain_openai langchain_huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v03kT8YGWrPx"
      },
      "source": [
        "## Setting Up Our Environment\n",
        "\n",
        "First, let's import all the necessary libraries. LangGraph provides the graph structure, while LangChain offers convenient interfaces for working with LLMs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import TypedDict, List, Dict, Any, Optional\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# âš ï¸ Do NOT hard-code your key in public code.\n",
        "# Set your OpenRouter key in an environment variable:\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"xxxxxxxx\"\n",
        "\n",
        "# Point the client to OpenRouterâ€™s endpoint\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "# Initialize the DeepSeek-v3 model via OpenRouter\n",
        "model = ChatOpenAI(\n",
        "    model=\"deepseek/deepseek-v3.1-terminus\",  # use the model slug shown in OpenRouter\n",
        "    temperature=0,\n",
        "    max_tokens=2000\n",
        ")\n"
      ],
      "metadata": {
        "id": "hjCCOCWSjBkH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTZgN66SWrPz"
      },
      "source": [
        "## Step 1: Define Our State\n",
        "\n",
        "In LangGraph, **State** is the central concept. It represents all the information that flows through our workflow.\n",
        "\n",
        "For Alfred's email processing system, we need to track:\n",
        "- The email being processed\n",
        "- Whether it's spam or not\n",
        "- The draft response (for legitimate emails)\n",
        "- Conversation history with the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y2pkEgmjWrPz"
      },
      "outputs": [],
      "source": [
        "class EmailState(TypedDict):\n",
        "    email: Dict[str, Any]\n",
        "    is_spam: Optional[bool]\n",
        "    spam_reason: Optional[str]\n",
        "    email_category: Optional[str]\n",
        "    email_draft: Optional[str]\n",
        "    messages: List[Dict[str, Any]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJM4IkckWrPz"
      },
      "source": [
        "## Step 2: Define Our Nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ99krFgWrP0",
        "outputId": "fcb80608-c98c-41b9-90d8-69904c451baa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7c049c1aca70>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def read_email(state: EmailState):\n",
        "    email = state[\"email\"]\n",
        "    print(f\"Alfred is processing an email from {email['sender']} with subject: {email['subject']}\")\n",
        "    return {}\n",
        "\n",
        "\n",
        "def classify_email(state: EmailState):\n",
        "    email = state[\"email\"]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "As Alfred the butler of Mr wayne and it's SECRET identity Batman, analyze this email and determine if it is spam or legitimate and should be brought to Mr wayne's attention.\n",
        "\n",
        "Email:\n",
        "From: {email['sender']}\n",
        "Subject: {email['subject']}\n",
        "Body: {email['body']}\n",
        "\n",
        "First, determine if this email is spam.\n",
        "answer with SPAM or HAM if it's legitimate. Only return the answer\n",
        "Answer :\n",
        "    \"\"\"\n",
        "    messages = [HumanMessage(content=prompt)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    response_text = response.content.lower()\n",
        "    print(response_text)\n",
        "    is_spam = \"spam\" in response_text and \"ham\" not in response_text\n",
        "\n",
        "    if not is_spam:\n",
        "        new_messages = state.get(\"messages\", []) + [\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "            {\"role\": \"assistant\", \"content\": response.content}\n",
        "        ]\n",
        "    else:\n",
        "        new_messages = state.get(\"messages\", [])\n",
        "\n",
        "    return {\n",
        "        \"is_spam\": is_spam,\n",
        "        \"messages\": new_messages\n",
        "    }\n",
        "\n",
        "\n",
        "def handle_spam(state: EmailState):\n",
        "    print(f\"Alfred has marked the email as spam.\")\n",
        "    print(\"The email has been moved to the spam folder.\")\n",
        "    return {}\n",
        "\n",
        "\n",
        "def drafting_response(state: EmailState):\n",
        "    email = state[\"email\"]\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "As Alfred the butler, draft a polite preliminary response to this email.\n",
        "\n",
        "Email:\n",
        "From: {email['sender']}\n",
        "Subject: {email['subject']}\n",
        "Body: {email['body']}\n",
        "\n",
        "Draft a brief, professional response that Mr. Wayne can review and personalize before sending.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [HumanMessage(content=prompt)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    new_messages = state.get(\"messages\", []) + [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"assistant\", \"content\": response.content}\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"email_draft\": response.content,\n",
        "        \"messages\": new_messages\n",
        "    }\n",
        "\n",
        "\n",
        "def notify_mr_wayne(state: EmailState):\n",
        "    email = state[\"email\"]\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(f\"Sir, you've received an email from {email['sender']}.\")\n",
        "    print(f\"Subject: {email['subject']}\")\n",
        "    print(\"\\nI've prepared a draft response for your review:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(state[\"email_draft\"])\n",
        "    print(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "    return {}\n",
        "\n",
        "\n",
        "# Define routing logic\n",
        "def route_email(state: EmailState) -> str:\n",
        "    if state[\"is_spam\"]:\n",
        "        return \"spam\"\n",
        "    else:\n",
        "        return \"legitimate\"\n",
        "\n",
        "\n",
        "# Create the graph\n",
        "email_graph = StateGraph(EmailState)\n",
        "\n",
        "# Add nodes\n",
        "email_graph.add_node(\"read_email\", read_email)  # the read_email node executes the read_mail function\n",
        "email_graph.add_node(\"classify_email\", classify_email)  # the classify_email node will execute the classify_email function\n",
        "email_graph.add_node(\"handle_spam\", handle_spam)  #same logic\n",
        "email_graph.add_node(\"drafting_response\", drafting_response)  #same logic\n",
        "email_graph.add_node(\"notify_mr_wayne\", notify_mr_wayne)  # same logic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45fXjSY6WrP0"
      },
      "source": [
        "## Step 3: Define Our Routing Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwvwORgJWrP0",
        "outputId": "3db0a3df-fd9a-4a6a-bab1-b51df95a78d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7c049c1aca70>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Add edges\n",
        "email_graph.add_edge(START, \"read_email\")  # After starting we go to the \"read_email\" node\n",
        "\n",
        "email_graph.add_edge(\"read_email\", \"classify_email\")  # after_reading we classify\n",
        "\n",
        "# Add conditional edges\n",
        "email_graph.add_conditional_edges(\n",
        "    \"classify_email\",  # after classify, we run the \"route_email\" function\"\n",
        "    route_email,\n",
        "    {\n",
        "        \"spam\": \"handle_spam\",  # if it return \"Spam\", we go the \"handle_span\" node\n",
        "        \"legitimate\": \"drafting_response\"  # and if it's legitimate, we go to the \"drafting response\" node\n",
        "    }\n",
        ")\n",
        "\n",
        "# Add final edges\n",
        "email_graph.add_edge(\"handle_spam\", END)  # after handling spam we always end\n",
        "email_graph.add_edge(\"drafting_response\", \"notify_mr_wayne\")\n",
        "email_graph.add_edge(\"notify_mr_wayne\", END)  # after notifyinf Me wayne, we can end  too\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WEyqqpHWrP1"
      },
      "source": [
        "## Step 4: Create the StateGraph and Define Edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x7EmCsp4WrP1"
      },
      "outputs": [],
      "source": [
        "# Compile the graph\n",
        "compiled_graph = email_graph.compile()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple fallback for immediate use\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Get the mermaid text\n",
        "mermaid_text = compiled_graph.get_graph().draw_mermaid()\n",
        "\n",
        "# Display as HTML with link to online viewer\n",
        "html_content = f\"\"\"\n",
        "<div style=\"border: 1px solid #ccc; padding: 10px; background: #f9f9f9;\">\n",
        "<h3>Graph Mermaid Code:</h3>\n",
        "<pre style=\"background: white; padding: 10px; overflow-x: auto;\">{mermaid_text}</pre>\n",
        "<p><a href=\"https://mermaid.live/\" target=\"_blank\">Click here to visualize this graph online</a></p>\n",
        "<p>Copy the code above and paste it into the Mermaid Live Editor</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "QnBeDxXAbEyM",
        "outputId": "f573b807-3ecd-46dd-c2b9-a03101b3c8a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<div style=\"border: 1px solid #ccc; padding: 10px; background: #f9f9f9;\">\n",
              "<h3>Graph Mermaid Code:</h3>\n",
              "<pre style=\"background: white; padding: 10px; overflow-x: auto;\">---\n",
              "config:\n",
              "  flowchart:\n",
              "    curve: linear\n",
              "---\n",
              "graph TD;\n",
              "\t__start__([<p>__start__</p>]):::first\n",
              "\tread_email(read_email)\n",
              "\tclassify_email(classify_email)\n",
              "\thandle_spam(handle_spam)\n",
              "\tdrafting_response(drafting_response)\n",
              "\tnotify_mr_wayne(notify_mr_wayne)\n",
              "\t__end__([<p>__end__</p>]):::last\n",
              "\t__start__ --> read_email;\n",
              "\tclassify_email -. &nbsp;legitimate&nbsp; .-> drafting_response;\n",
              "\tclassify_email -. &nbsp;spam&nbsp; .-> handle_spam;\n",
              "\tdrafting_response --> notify_mr_wayne;\n",
              "\tread_email --> classify_email;\n",
              "\thandle_spam --> __end__;\n",
              "\tnotify_mr_wayne --> __end__;\n",
              "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
              "\tclassDef first fill-opacity:0\n",
              "\tclassDef last fill:#bfb6fc\n",
              "</pre>\n",
              "<p><a href=\"https://mermaid.live/\" target=\"_blank\">Click here to visualize this graph online</a></p>\n",
              "<p>Copy the code above and paste it into the Mermaid Live Editor</p>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pyppeteer install\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq4ALKL3Yq_y",
        "outputId": "0060a35a-fb54-4e68-af22-b408b06421f2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: No module named pyppeteer.__main__; 'pyppeteer' is a package and cannot be directly executed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN96EGkkWrP2",
        "outputId": "cf7743ea-42f8-42b8-d683-2c7e27a43f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing legitimate email...\n",
            "Alfred is processing an email from Joker with subject: Found you Batman ! \n",
            "ham\n",
            "\n",
            "==================================================\n",
            "Sir, you've received an email from Joker.\n",
            "Subject: Found you Batman ! \n",
            "\n",
            "I've prepared a draft response for your review:\n",
            "--------------------------------------------------\n",
            "Of course. Here is a preliminary draft, composed in a tone befitting the Wayne household.\n",
            "\n",
            "***\n",
            "\n",
            "**DRAFT FOR REVIEW**\n",
            "\n",
            "**Subject: Re: Found you Batman !**\n",
            "\n",
            "Dear Mr. Joker,\n",
            "\n",
            "Thank you for your email, which has been received by the office of Mr. Bruce Wayne.\n",
            "\n",
            "Mr. Wayne is currently occupied with the operations of Wayne Enterprises and his philanthropic endeavors in Gotham. The assertion contained in your message has been noted; however, the fanciful nature of its content does not align with any recognizable reality.\n",
            "\n",
            "We trust this clarifies the matter.\n",
            "\n",
            "Respectfully,\n",
            "\n",
            "**Alfred Pennyworth**\n",
            "Executive Butler, Wayne Manor\n",
            "==================================================\n",
            "\n",
            "\n",
            "Processing spam email...\n",
            "Alfred is processing an email from Crypto bro with subject: The best investment of 2025\n",
            "spam\n",
            "Alfred has marked the email as spam.\n",
            "The email has been moved to the spam folder.\n"
          ]
        }
      ],
      "source": [
        " # Example emails for testing\n",
        "legitimate_email = {\n",
        "    \"sender\": \"Joker\",\n",
        "    \"subject\": \"Found you Batman ! \",\n",
        "    \"body\": \"Mr. Wayne,I found your secret identity ! I know you're batman ! Ther's no denying it, I have proof of that and I'm coming to find you soon. I'll get my revenge. JOKER\"\n",
        "}\n",
        "\n",
        "spam_email = {\n",
        "    \"sender\": \"Crypto bro\",\n",
        "    \"subject\": \"The best investment of 2025\",\n",
        "    \"body\": \"Mr Wayne, I just launched an ALT coin and want you to buy some !\"\n",
        "}\n",
        "# Process legitimate email\n",
        "print(\"\\nProcessing legitimate email...\")\n",
        "legitimate_result = compiled_graph.invoke({\n",
        "    \"email\": legitimate_email,\n",
        "    \"is_spam\": None,\n",
        "    \"spam_reason\": None,\n",
        "    \"email_category\": None,\n",
        "    \"email_draft\": None,\n",
        "    \"messages\": []\n",
        "})\n",
        "\n",
        "# Process spam email\n",
        "print(\"\\nProcessing spam email...\")\n",
        "spam_result = compiled_graph.invoke({\n",
        "    \"email\": spam_email,\n",
        "    \"is_spam\": None,\n",
        "    \"spam_reason\": None,\n",
        "    \"email_category\": None,\n",
        "    \"email_draft\": None,\n",
        "    \"messages\": []\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF4htUAMWrP2"
      },
      "source": [
        "## Step 5: Inspecting Our Mail Sorting Agent with Langfuse ğŸ“¡\n",
        "\n",
        "As Alfred fine-tunes the Main Sorting Agent, he's growing weary of debugging its runs. Agents, by nature, are unpredictable and difficult to inspect. But since he aims to build the ultimate Spam Detection Agent and deploy it in production, he needs robust traceability for future monitoring and analysis.\n",
        "\n",
        "To do this, Alfred can use an observability tool such as [Langfuse](https://langfuse.com/) to trace and monitor the inner steps of the agent.\n",
        "\n",
        "First, we need to install the necessary dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50MHDY5GWrP3",
        "outputId": "c47b4445-d29b-4cd0-8cab-c6d27e6ad960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/348.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m348.2/348.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m348.5/348.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/65.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/72.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/131.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/208.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q langfuse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DflRHm-7WrP3"
      },
      "source": [
        "Next, we set the Langfuse API keys and host address as environment variables. You can get your Langfuse credentials by signing up for [Langfuse Cloud](https://cloud.langfuse.com) or [self-hosting Langfuse](https://langfuse.com/self-hosting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PLxNXxtbWrP3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page: https://cloud.langfuse.com\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
        "os.environ[\"LANGFUSE_HOST\"] = \"https://cloud.langfuse.com\"  # ğŸ‡ªğŸ‡º EU region\n",
        "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\" # ğŸ‡ºğŸ‡¸ US region"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH3NoJ5HWrP4"
      },
      "source": [
        "Now, we configure the [Langfuse `callback_handler`](https://langfuse.com/docs/integrations/langchain/tracing#add-langfuse-to-your-langchain-application)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B8-_EHM8WrP4"
      },
      "outputs": [],
      "source": [
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "# Initialize Langfuse CallbackHandler for LangGraph/Langchain (tracing)\n",
        "langfuse_handler = CallbackHandler()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfea2476"
      },
      "source": [
        "from langfuse.langchain import CallbackHandler\n",
        "\n",
        "# Initialize Langfuse CallbackHandler for LangGraph/Langchain (tracing)\n",
        "langfuse_handler = CallbackHandler()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b4bc176",
        "outputId": "2e02f092-da9d-45de-b0af-b59a9b745470"
      },
      "source": [
        "%pip install langchain"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.31)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
            "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
            "Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: greenlet, SQLAlchemy, langchain-text-splitters, langchain\n",
            "Successfully installed SQLAlchemy-2.0.43 greenlet-3.2.4 langchain-0.3.27 langchain-text-splitters-0.3.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQFTTXNaWrP4"
      },
      "source": [
        "We then add `config={\"callbacks\": [langfuse_handler]}` to the invocation of the agents and run them again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO5KI_-_WrP5",
        "outputId": "8c4df4d0-2837-4f19-b432-d343b7ef3920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing legitimate email...\n",
            "Alfred is processing an email from Joker with subject: Found you Batman ! \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:opentelemetry.exporter.otlp.proto.http.trace_exporter:Failed to export span batch code: 401, reason: {\"message\":\"Invalid credentials. Confirm that you've configured the correct host.\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham\n",
            "\n",
            "==================================================\n",
            "Sir, you've received an email from Joker.\n",
            "Subject: Found you Batman ! \n",
            "\n",
            "I've prepared a draft response for your review:\n",
            "--------------------------------------------------\n",
            "Of course. Here is a preliminary draft, composed in a tone befitting the Wayne household.\n",
            "\n",
            "***\n",
            "\n",
            "**Subject: Re: Found you Batman !**\n",
            "\n",
            "Dear Sir or Madam,\n",
            "\n",
            "Thank you for your recent correspondence addressed to Mr. Wayne.\n",
            "\n",
            "Please be advised that all communications for Mr. Wayne are received and reviewed by his office. We treat all messages with the utmost discretion.\n",
            "\n",
            "The assertions contained in your email are, of course, categorically denied. Such claims are frivolous and without merit.\n",
            "\n",
            "This matter is now considered closed, and no further correspondence will be entered into on this subject.\n",
            "\n",
            "Respectfully,\n",
            "\n",
            "**Alfred Pennyworth**\n",
            "Estate Manager for Mr. Bruce Wayne\n",
            "==================================================\n",
            "\n",
            "\n",
            "Processing spam email...\n",
            "Alfred is processing an email from Crypto bro with subject: The best investment of 2025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:opentelemetry.exporter.otlp.proto.http.trace_exporter:Failed to export span batch code: 401, reason: {\"message\":\"Invalid credentials. Confirm that you've configured the correct host.\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n",
            "Alfred has marked the email as spam.\n",
            "The email has been moved to the spam folder.\n"
          ]
        }
      ],
      "source": [
        "# Process legitimate email\n",
        "print(\"\\nProcessing legitimate email...\")\n",
        "legitimate_result = compiled_graph.invoke(\n",
        "    input={\n",
        "        \"email\": legitimate_email,\n",
        "        \"is_spam\": None,\n",
        "        \"draft_response\": None,\n",
        "        \"messages\": []\n",
        "    },\n",
        "    config={\"callbacks\": [langfuse_handler]}\n",
        ")\n",
        "\n",
        "# Process spam email\n",
        "print(\"\\nProcessing spam email...\")\n",
        "spam_result = compiled_graph.invoke(\n",
        "    input={\n",
        "        \"email\": spam_email,\n",
        "        \"is_spam\": None,\n",
        "        \"draft_response\": None,\n",
        "        \"messages\": []\n",
        "    },\n",
        "    config={\"callbacks\": [langfuse_handler]}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSfHdESWrP5"
      },
      "source": [
        "Alfred is now connected ğŸ”Œ! The runs from LangGraph are being logged in Langfuse, giving him full visibility into the agent's behavior. With this setup, he's ready to revisit previous runs and refine his Mail Sorting Agent even further.\n",
        "\n",
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/huggingface-agent-course/langgraph-trace-legit.png)\n",
        "\n",
        "_[Public link to the trace with the legit email](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/f5d6d72e-20af-4357-b232-af44c3728a7b?timestamp=2025-03-17T10%3A13%3A28.413Z&observation=6997ba69-043f-4f77-9445-700a033afba1)_\n",
        "\n",
        "![Example trace in Langfuse](https://langfuse.com/images/cookbook/huggingface-agent-course/langgraph-trace-spam.png)\n",
        "\n",
        "_[Public link to the trace with the spam email](https://langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/6e498053-fee4-41fd-b1ab-d534aca15f82?timestamp=2025-03-17T10%3A13%3A30.884Z&observation=84770fc8-4276-4720-914f-bf52738d44ba)_\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
